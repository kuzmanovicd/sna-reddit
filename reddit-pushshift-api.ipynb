{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json, time, datetime, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "ROOT_PATH = 'data/'\n",
    "DATA_PATH = '-'.join(str(x) for x in [(now.year), now.month, now.day])\n",
    "\n",
    "def print_time(msg, unix):\n",
    "    print(msg, time.ctime(int(unix)))\n",
    "    \n",
    "def get_readable_time(unix):\n",
    "    return [time.ctime(int(u)) for u in unix]\n",
    "\n",
    "def print_progress(iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, errors = 0, fill = '+'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s - errors: %s' % (prefix, bar, percent, suffix, errors), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == (total - 2): \n",
    "        print()\n",
    "        \n",
    "def retrieve_data(url):\n",
    "    r = requests.get(url)    \n",
    "    if(r.status_code >= 400):\n",
    "        print('Sleeping for one minute...')\n",
    "        time.sleep(60)\n",
    "        return retrieve_data(url)\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "def query_data(url, *args, **kwargs):\n",
    "    payload = {}\n",
    "    if kwargs is not None:\n",
    "        for key, value in kwargs.items():\n",
    "            payload[key] = value\n",
    "    print(payload)\n",
    "    \n",
    "    new_url = url\n",
    "    if args is not None:\n",
    "        for value in args:\n",
    "            new_url = new_url + value + '/'\n",
    "    print(new_url)\n",
    "    r = requests.get(url, params=payload)    \n",
    "    if(400 <= r.status_code < 500):\n",
    "        print(r.status_code, ' Trying again')\n",
    "        time.sleep(1)\n",
    "        return query_data(url, *args, **kwargs)\n",
    "    elif r.status_code >= 500:\n",
    "        print('Server error:', r.status_code, r.content)\n",
    "        print(r.request.body, r.request.headers)\n",
    "        return None\n",
    "    else:\n",
    "        return r.json()\n",
    "\n",
    "def query_data_future(session, url, *args, **kwargs):\n",
    "    payload = {}\n",
    "    if kwargs is not None:\n",
    "        for key, value in kwargs.items():\n",
    "            payload[key] = value\n",
    "    #print(payload)\n",
    "    new_url = url\n",
    "    if args is not None:\n",
    "        for value in args:\n",
    "            new_url = new_url + value + '/'\n",
    "    #print(new_url)\n",
    "    r = session.get(url, params=payload)    \n",
    "    return r\n",
    "\n",
    "def save_data(df, name):\n",
    "    if os.path.isdir(ROOT_PATH + DATA_PATH) == False:\n",
    "        os.mkdir(ROOT_PATH + DATA_PATH)\n",
    "        print('Creating', DATA_PATH, 'directory...')\n",
    "    df.to_pickle(ROOT_PATH + DATA_PATH + '/' + name)\n",
    "    \n",
    "\n",
    "URL_COM = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "URL_SUB = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "\n",
    "QUERIES = ['bitcoin', 'ethereum', 'crypto', 'ripple', 'litecoin', 'btc']\n",
    "DROP_SUBREDDITS = ['AskReddit', 'news', 'Sexsells']\n",
    "TOP_SUBREDDITS_TO_QUERY = 20\n",
    "DOWNLOAD_NEW_DATA = False\n",
    "LOAD_DATA_PATH = '2018-2-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bg_count  doc_count                  key     score\n",
      "0    1521817      40812              Bitcoin   2.68179\n",
      "1     676344      27633                  btc   4.08564\n",
      "2    1559250      22551       CryptoCurrency   1.44627\n",
      "3     153798       9978           BitcoinAll   6.48773\n",
      "4      45201       6630  noncensored_bitcoin  14.66782\n",
      "5     285720       6140       BitcoinMarkets   2.14896\n",
      "6     740446       5661            ethtrader   0.76454\n",
      "7      62712       3596             Buttcoin   5.73415\n",
      "8     290118       2595               Ripple   0.89446\n",
      "9    1350445       1835         pcmasterrace   0.13588\n",
      "10    180451       1797            RaiBlocks   0.99584\n",
      "11     26680       1619             xyMarket   6.06822\n",
      "12    179150       1560            investing   0.87078\n",
      "13     52570       1440     BitcoinBeginners   2.73920\n",
      "14    236659       1408             litecoin   0.59495\n",
      "15    623007       1376           technology   0.22086\n",
      "16   3211589       1342            worldnews   0.04179\n",
      "17    109448       2767             ethereum   2.52814\n",
      "18     83739        492          EtherMining   0.58754\n",
      "19    121796        477                  NEO   0.39164\n",
      "20    127896        466               Tronix   0.36436\n",
      "21     72056        434              Vechain   0.60231\n",
      "22     30856        405                  eos   1.31255\n",
      "23     53744        403             omise_go   0.74985\n",
      "24     83405        400        CryptoMarkets   0.47959\n",
      "25    107010        379              Stellar   0.35417\n",
      "26     17812        936                  XRP   5.25488\n",
      "27      4784        155              gatehub   3.23997\n",
      "28     36444        130         ledgerwallet   0.35671\n",
      "29    109137       1035      LitecoinMarkets   0.94835\n",
      "30     98251        297             dogecoin   0.30229\n",
      "31    180942        186            garlicoin   0.10280\n",
      "32     91713        153             vertcoin   0.16682\n",
      "33     81269        106             CoinBase   0.13043\n",
      "34    103419       2347             NiceHash   2.26941\n",
      "35     32497       1197             RaiTrade   3.68342\n",
      "36     58541        904          Bitcoincash   1.54422\n",
      "37      9976        891     BitGrailExchange   8.93144\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top 10 subreddits for each query\n",
    "popular_subreddits = []\n",
    "popular_subreddits_df = pd.DataFrame()\n",
    "if DOWNLOAD_NEW_DATA:    \n",
    "    for query in QUERIES:\n",
    "        tmp_data = query_data(URL_COM, q=query, size=0, aggs='subreddit', after='30d')['aggs']['subreddit'][0:TOP_SUBREDDITS_TO_QUERY]\n",
    "        popular_subreddits.extend(tmp_data)\n",
    "\n",
    "    popular_subreddits_df = pd.DataFrame(popular_subreddits)\n",
    "    popular_subreddits_df = popular_subreddits_df.drop_duplicates('key').reset_index(drop=True)\n",
    "    popular_subreddits_df = popular_subreddits_df[~popular_subreddits_df['key'].isin(DROP_SUBREDDITS)].reset_index(drop=True)\n",
    "    save_data(popular_subreddits_df, 'popular_subreddits.pkl')\n",
    "else:\n",
    "    popular_subreddits_df = pd.read_pickle(ROOT_PATH + LOAD_DATA_PATH + '/' + 'popular_subreddits.pkl')\n",
    "    popular_subreddits = popular_subreddits_df.to_dict()\n",
    "    \n",
    "#TEMP - DELETE\n",
    "print(popular_subreddits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissions successfully loaded\n"
     ]
    }
   ],
   "source": [
    "TIME_STEP = 6\n",
    "START_AFTER = 24\n",
    "DAYS = 30\n",
    "\n",
    "sub_fields = ['id', 'author', 'subreddit', 'score', 'num_comments']\n",
    "sub_fields_save = ['author', 'subreddit_id', 'subreddit', 'score', 'num_comments', 'id', 'created_utc', 'retrieved_on', 'num_crossposts', 'title', 'url', 'stickied', 'pinned', 'gilded', ]\n",
    "\n",
    "submissions = []\n",
    "submissions_df = pd.DataFrame()\n",
    "fut = []\n",
    "errors = []\n",
    "if DOWNLOAD_NEW_DATA:\n",
    "    with FuturesSession(max_workers=10) as session:\n",
    "        for i, sub in popular_subreddits_df.iterrows(): \n",
    "            for idx in range(int(24/TIME_STEP * DAYS)):\n",
    "                _before = str(TIME_STEP*idx + START_AFTER) + 'h'\n",
    "                _after = str(TIME_STEP*(idx+1) + START_AFTER) + 'h'\n",
    "                fut.append(query_data_future(session, URL_SUB, subreddit=sub['key'], size=500, before=_before, after=_after))\n",
    "            if i > 1:\n",
    "                pass\n",
    "                #break\n",
    "\n",
    "        for idx in range(len(fut)):\n",
    "            print_progress(idx, len(fut), prefix = 'Start', suffix = str(idx) + '/' + str(len(fut)), length=50, errors=len(errors))\n",
    "            tmp = {}\n",
    "            try:\n",
    "                tmp = fut[idx].result().json()['data']\n",
    "                submissions.append(pd.DataFrame(tmp))\n",
    "                if idx and idx % 30 == 0:\n",
    "                    submissions_df = pd.concat(submissions)[sub_fields_save].query('num_comments>1').reset_index(drop=True)\n",
    "                    save_data(submissions_df, 'submissions.pkl')\n",
    "            except:\n",
    "                errors.append(fut[idx]) \n",
    "\n",
    "        submissions_df = pd.concat(submissions)[sub_fields_save].query('num_comments>1').reset_index(drop=True)\n",
    "        save_data(submissions_df, 'submissions.pkl')\n",
    "        print('Downloading submissions done...')\n",
    "else:\n",
    "    submissions_df = pd.read_pickle(ROOT_PATH + LOAD_DATA_PATH + '/' + 'submissions.pkl')\n",
    "    print('Submissions successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 104127 entries, 0 to 104126\n",
      "Data columns (total 14 columns):\n",
      "author            104127 non-null object\n",
      "subreddit_id      104127 non-null object\n",
      "subreddit         104127 non-null object\n",
      "score             104127 non-null int64\n",
      "num_comments      104127 non-null int64\n",
      "id                104127 non-null object\n",
      "created_utc       104127 non-null int64\n",
      "retrieved_on      104127 non-null int64\n",
      "num_crossposts    104127 non-null int64\n",
      "title             104127 non-null object\n",
      "url               104127 non-null object\n",
      "stickied          104127 non-null bool\n",
      "pinned            104127 non-null bool\n",
      "gilded            362 non-null float64\n",
      "dtypes: bool(2), float64(1), int64(5), object(6)\n",
      "memory usage: 9.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "submissions_df = submissions_df.sort_values(by=['num_comments'], ascending=False).query('subreddit!=\"worldnews\"&author!=\"AutoModerator\"&author!=\"[deleted]\"').reset_index(drop=True)\n",
    "#submissions_df = submissions_df.query('subreddit==\"Bitcoin\"&num_comments>25').reset_index(drop=True)\n",
    "#print(submissions_df[sub_fields])\n",
    "print(submissions_df.info())\n",
    "#print(submissions_df[1:104000]['num_comments'].sum())\n",
    "#print(submissions_df[submissions_df['author'].str.contains('Bot')][sub_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment ids successfully loaded...\n",
      "1926303\n"
     ]
    }
   ],
   "source": [
    "#get comment ids for every submission obtained above\n",
    "#concurrent calls\n",
    "fut = []\n",
    "comment_ids = []\n",
    "comment_ids_df = pd.DataFrame()\n",
    "errors = []\n",
    "IDS_DOWNLOAD = False\n",
    "if IDS_DOWNLOAD:\n",
    "    with FuturesSession(max_workers=10) as session:\n",
    "        print('Starting...')\n",
    "        for i, sub in submissions_df.iterrows():\n",
    "            fut.append(session.get('https://api.pushshift.io/reddit/submission/comment_ids/' + sub.loc['id']))\n",
    "\n",
    "        for i in range(len(fut)):\n",
    "            print_progress(i, len(fut), prefix = 'Start', suffix = str(i) + '/' + str(len(fut)), length=50, errors=len(errors))\n",
    "            try:\n",
    "                tmp = fut[i].result()\n",
    "                tmp = tmp.json()['data']\n",
    "                comment_ids.extend(tmp)\n",
    "            except:\n",
    "                print(fut[i].result().status_code)\n",
    "                errors.append(fut[i])\n",
    "                \n",
    "        comment_ids_df = pd.DataFrame({\n",
    "            'id': comment_ids\n",
    "        })\n",
    "        save_data(comment_ids_df, 'comment_ids.pkl')\n",
    "else:\n",
    "    comment_ids_df = pd.read_pickle(ROOT_PATH + LOAD_DATA_PATH + '/' + 'comment_ids_39k.pkl')\n",
    "    comment_ids = comment_ids_df['id'].values.tolist()\n",
    "    print('Comment ids successfully loaded...')\n",
    "\n",
    "print(len(comment_ids))\n",
    "#print('https://api.pushshift.io/reddit/search/comment/?ids=' + ','.join(comment_ids[0:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 2018-2-6 directory...---------------------------| 2.6% 100/3853 - errors: 0\n",
      "Start |+++++++++++++++++++++++++++++++++++++++++++++++++-| 99.9% 3851/3853 - errors: 0\n",
      "<class 'pandas.core.frame.DataFrame'>+++++++++++++++++++-| 100.0% 3852/3853 - errors: 0\n",
      "RangeIndex: 1926302 entries, 0 to 1926301\n",
      "Data columns (total 31 columns):\n",
      "approved_at_utc                  object\n",
      "author                           object\n",
      "author_cakeday                   object\n",
      "author_flair_background_color    object\n",
      "author_flair_css_class           object\n",
      "author_flair_richtext            object\n",
      "author_flair_text                object\n",
      "author_flair_text_color          object\n",
      "author_flair_type                object\n",
      "banned_at_utc                    object\n",
      "body                             object\n",
      "can_mod_post                     bool\n",
      "collapsed                        bool\n",
      "collapsed_reason                 object\n",
      "created_utc                      int64\n",
      "distinguished                    object\n",
      "edited                           object\n",
      "id                               object\n",
      "is_submitter                     bool\n",
      "link_id                          object\n",
      "mod_note                         object\n",
      "mod_reason_by                    object\n",
      "mod_reason_title                 object\n",
      "parent_id                        object\n",
      "permalink                        object\n",
      "retrieved_on                     int64\n",
      "rte_mode                         object\n",
      "score                            int64\n",
      "stickied                         bool\n",
      "subreddit                        object\n",
      "subreddit_id                     object\n",
      "dtypes: bool(4), int64(3), object(24)\n",
      "memory usage: 404.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#get data for every comment\n",
    "NUM_CONCAT = 500\n",
    "fut = []\n",
    "errors = []\n",
    "comments = []\n",
    "comments_df = pd.DataFrame()\n",
    "if False:\n",
    "    with FuturesSession(max_workers=5) as session:\n",
    "        _done = 0\n",
    "        idx = 0\n",
    "        while _done != -1:\n",
    "            _end = _done + NUM_CONCAT\n",
    "            if _end >= len(comment_ids) + 1:\n",
    "                _end = -1\n",
    "            fut.append(session.get('https://api.pushshift.io/reddit/search/comment/?ids=' + ','.join(comment_ids[_done:_end])))\n",
    "            _done = _end\n",
    "\n",
    "        for i in range(len(fut)):\n",
    "            print_progress(i, len(fut), prefix = 'Start', suffix = str(i) + '/' + str(len(fut)), length=50, errors=len(errors))\n",
    "            try:\n",
    "                tmp = fut[i].result()\n",
    "                tmp = tmp.json()['data']\n",
    "                comments.extend(tmp)\n",
    "                \n",
    "                if i and i % 100 == 0:\n",
    "                    comments_df = pd.DataFrame(comments)\n",
    "                    save_data(comments_df, 'comments.pkl')\n",
    "            except:\n",
    "                errors.append(fut[i])\n",
    "        \n",
    "        comments_df = pd.DataFrame(comments)\n",
    "        save_data(comments_df, 'comments.pkl')\n",
    "else:\n",
    "    comments_df = pd.read_pickle(ROOT_PATH + LOAD_DATA_PATH + '/' + 'comments.pkl')\n",
    "    print('Comments successfully loaded...')\n",
    "    \n",
    "print(comments_df.info())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type prefixes\n",
    "t1_\tComment\n",
    "t2_\tAccount\n",
    "t3_\tLink\n",
    "t4_\tMessage\n",
    "t5_\tSubreddit\n",
    "t6_\tAward"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "author_1 -> comment(score, ups) -> author_2\n",
    "\n",
    "simplified\n",
    "\n",
    "author_1 -> comment_on_sub -> author_2\n",
    "author_1 -> reply_on_comment -> author_2\n",
    "\n",
    "author_1 -> submission -> subreddit\n",
    "author_1 -> comment_on_sub -> subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approved_at_utc': None, 'author': 'Asdn1220', 'author_flair_background_color': '', 'author_flair_css_class': None, 'author_flair_richtext': [], 'author_flair_text': None, 'author_flair_text_color': 'dark', 'author_flair_type': 'text', 'banned_at_utc': None, 'body': 'I am panicking', 'can_mod_post': False, 'collapsed': False, 'collapsed_reason': None, 'created_utc': 1517479281, 'distinguished': None, 'edited': False, 'id': 'dtkd1na', 'is_submitter': False, 'link_id': 't3_7uhqjf', 'mod_note': None, 'mod_reason_by': None, 'mod_reason_title': None, 'parent_id': 't3_7uhqjf', 'permalink': '/r/Bitcoin/comments/7uhqjf/daily_discussion_february_01_2018/dtkd1na/', 'retrieved_on': 1517479283, 'rte_mode': 'markdown', 'score': 1, 'stickied': False, 'subreddit': 'Bitcoin', 'subreddit_id': 't5_2s3qj'}\n"
     ]
    }
   ],
   "source": [
    "comments_df = pd.DataFrame(comment_data)\n",
    "print(comment_data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
